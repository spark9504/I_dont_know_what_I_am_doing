{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lstm_with_nested_val.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyNSmWbNuW8puyBdRHSMhxOz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# 1. Import Libraries"],"metadata":{"id":"bojardNbzGOD"}},{"cell_type":"code","metadata":{"id":"iAFyic3UsWWr"},"source":["import os\n","import pandas as pd\n","import numpy as np\n","from pandas import read_csv\n","from datetime import datetime\n","from pandas import DataFrame\n","from sklearn import preprocessing\n","from sklearn.metrics import mean_squared_error, accuracy_score\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import KFold, train_test_split, TimeSeriesSplit\n","from matplotlib import pyplot as plt\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n","from tensorflow.keras.optimizers import Adam\n","from keras.models import load_model\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2. Define Functions"],"metadata":{"id":"E7mwOAmNzO3w"}},{"cell_type":"code","source":["def dataset_builder(df, y_index, history_points):\n","    arr = df.to_numpy()\n","    arr_normalizer = preprocessing.MinMaxScaler() # 데이터를 0~1 범위로 점철되게 하는 함수 call\n","    arr_normalized = arr_normalizer.fit_transform(arr) # 데이터를 0~1 범위로 점철되게 함수 수행\n","\n","    # train data x \n","    # 변수를 가지고 오되, 관찰일수 만큼 누적해서 쌓는다. (열방향으로)\n","    x_set = np.array([arr_normalized[i:i + history_points].copy() for i in range(len(arr_normalized) - history_points)]) \n","    print('x_set: ', x_set.shape)\n","        \n","    # y data set\n","    y_set = np.array([arr_normalized[:, y_index][i + history_points].copy() for i in range(len(arr_normalized) - history_points)])\n","    y_set = np.expand_dims(y_set, -1) # 1XN 벡터 -> NX1 벡터로\n","    print('y_set: ', y_set.shape)\n","\n","    y_set_unnorm = np.array([arr[:, y_index][i + history_points].copy() for i in range(len(arr) - history_points)])\n","    y_set_unnorm = np.expand_dims(y_set_unnorm, -1) # 1XN 벡터 -> NX1 벡터로\n","\n","    y_set_normalizer = preprocessing.MinMaxScaler()\n","    y_set_normalizer.fit(y_set_unnorm)\n","\n","    # 인풋 X : 그 이전의 변수 (from T = -50 to T = -1)\n","    # 아웃풋 y : 예측하고자 하는 변수 T = 0\n","\n","    assert x_set.shape[0] == y_set.shape[0]\n","    return x_set, y_set, y_set_unnorm, y_set_normalizer\n","\n","def split_train_test(x_set, y_set, y_set_unnorm, n_test):\n","    \n","    x_train = x_set[:-n_test]\n","    y_train = y_set[:-n_test]\n","    \n","    x_test = x_set[-n_test:]\n","    y_test = y_set[-n_test:]\n","\n","    y_set_unnorm_test = y_set_unnorm[-n_test:]\n","    \n","    print('x_train: ', x_train.shape)\n","    print('x_test: ', x_test.shape)\n","    print('y_train: ', y_train.shape)\n","    print('y_test: ', y_test.shape)\n","    \n","    return x_train, y_train, x_test, y_test, y_set_unnorm_test\n","\n","def model_builder(x_train):\n","    model = keras.Sequential()\n","    model.add(layers.Input(batch_shape=(None, x_train.shape[1], x_train.shape[2])))\n","    model.add(layers.LSTM(64, return_sequences=True))\n","    model.add(layers.LSTM(32, return_sequences=False))\n","    model.add(layers.Dropout(0.2))\n","    model.add(layers.Dense(16, activation = 'relu'))\n","    model.add(layers.Dense(1, activation = 'softmax'))\n","    model.compile(loss='mean_squared_error', optimizer='adam')\n","    model.summary()    \n","    return model"],"metadata":{"id":"PFPM6AVBxcUO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. Import Dataset and Preprocessing\n","    file location needs to be changed "],"metadata":{"id":"eFKroQphzVQM"}},{"cell_type":"code","metadata":{"id":"ug42rr7Xslut"},"source":["df = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/predict_220720_1.xlsx')\n","x_set, y_set, y_set_unnorm, y_set_normalizer = dataset_builder(df, y_index=0, history_points=50)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n_test = 24\n","x_train, y_train, x_test, y_test, y_set_unnorm_test = split_train_test(x_set, y_set, y_set_unnorm, n_test)"],"metadata":{"id":"gplnO--WxKbQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4. Build Model and Training"],"metadata":{"id":"j7q7PM2lz_ei"}},{"cell_type":"code","source":["model = model_builder(x_train)"],"metadata":{"id":"Xy5RuyFtxMZG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["early_stop = EarlyStopping(monitor='val_loss', patience=15)\n","model_path = '/content/drive/MyDrive/Colab Notebooks/saved_models'\n","filename = os.path.join(model_path, 'tmp_checkpoint.h5')\n","checkpoint = ModelCheckpoint(filepath=filename, \n","                             save_best_only=True, \n","                             monitor='val_loss', \n","                             verbose=1)"],"metadata":{"id":"ZPTRhXbixOvj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tscv = TimeSeriesSplit(gap=0, max_train_size=None, n_splits=10, test_size=None)\n","# i = 0\n","rmse = []\n","for train_index, val_index in tscv.split(x_train):\n","    print(\"TRAIN:\", train_index, \"VAL:\", val_index)\n","    x_train_train, x_train_val = x_train[train_index], x_train[val_index]\n","    y_train_train, y_train_val = y_train[train_index], y_train[val_index]\n","    \n","    with tf.device(\"/device:GPU:0\"):\n","        history = model.fit(x_train_train,\n","                            y_train_train,\n","                            epochs=200,\n","                            batch_size=10,\n","                            shuffle=False,\n","                            validation_data=(x_train_val,y_train_val),\n","                            callbacks=[checkpoint, early_stop],\n","                            verbose=1)\n","    rmse.append(model.evaluate(x_train_val))\n","    # history_path = os.path.join(model_path, 'model_' + str(i+1) + '.h5')\n","    # model.save(history_path)\n","    # i += 1"],"metadata":{"id":"afyR5h25xQvn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rmse"],"metadata":{"id":"utRaGCBbxYRm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5. Prediction and Plot"],"metadata":{"id":"_b95xzvZ0cLI"}},{"cell_type":"code","source":["model = keras.models.load_model(filename)\n","pred = model.predict(x_test)\n","print(pred.shape, y_test.shape)\n","y_true_plot = np.concatenate((y_train, y_test), axis=0)\n","y_pred_plot = np.concatenate((y_train, pred), axis=0)"],"metadata":{"id":"0DzfH9rgxZZc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(12, 9))\n","plt.plot(y_true_plot, label = 'actual')\n","plt.plot(y_pred_plot, label = 'prediction')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"zw_V7852xa5O"},"execution_count":null,"outputs":[]}]}